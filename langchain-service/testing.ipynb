{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661842c6",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "1. Load the **PDFs or Markdown** files in Order\n",
    "2. Extract the **data/content** from the PDF\n",
    "3. Then perform chunking - *Because the context window for the LLMs are small*\n",
    "4. Then pass it to the **Embedding Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e8ac08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sayan\\OneDrive\\Desktop\\internship-project\\enterprise-knowledge-copilot\\langchain-service\\.langchain-venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 1. Loading of the PDFs\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e85ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "FILE_PATH = \"C:/Users/sayan/OneDrive/Desktop/internship-project/enterprise-knowledge-copilot/data\"\n",
    "\n",
    "# Function to load PDF files from a specified directory\n",
    "def load_pdf_file(file_path=FILE_PATH):\n",
    "    if not os.path.isdir(file_path):\n",
    "        print(f\"Error: The provided path '{file_path}' is not a directory.\")\n",
    "        return []\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "        file_path,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader,\n",
    "        recursive=True,\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Function to load Markdown files from a specified directory\n",
    "def load_markdown_file(file_path=FILE_PATH):\n",
    "    if not os.path.isdir(file_path):\n",
    "        print(f\"Error: The provided path '{file_path}' is not a directory.\")\n",
    "        return []\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "        file_path,\n",
    "        glob=\"**/*.md\",           \n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs={\"encoding\": \"utf-8\"}\n",
    "    )\n",
    "\n",
    "    docs = loader.load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b72cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to clean markdown text\n",
    "def clean_markdown(md_text: str) -> str:\n",
    "    # 1. Remove YAML front matter\n",
    "    md_text = re.sub(r\"^---.*?---\", \"\", md_text, flags=re.DOTALL)\n",
    "\n",
    "    # 2. Convert markdown → HTML\n",
    "    html = markdown.markdown(md_text)\n",
    "\n",
    "    # 3. Parse HTML\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # 4. Remove unwanted tags\n",
    "    for tag in soup([\"script\", \"style\", \"iframe\", \"img\", \"table\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # 5. Get text\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "    # 6. Remove markdown links but keep text\n",
    "    text = re.sub(r\"\\[(.*?)\\]\\(.*?\\)\", r\"\\1\", text)\n",
    "\n",
    "    # 7. Normalize whitespace\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "459975d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters documents to only include page_content and source metadata.\n",
    "def filter_to_minimal_docs(docs):\n",
    "    minimal_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        full_path = doc.metadata['source']\n",
    "        file_name = os.path.basename(full_path)\n",
    "\n",
    "        minimal_doc = Document(\n",
    "            page_content=clean_markdown(doc.page_content),\n",
    "            metadata={\n",
    "                \"source\": file_name\n",
    "            }\n",
    "        )\n",
    "        minimal_docs.append(minimal_doc)\n",
    "\n",
    "    return minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b0c5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into smaller chunks\n",
    "def text_split(minimal_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(minimal_docs)\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62cb2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Markdown files\n",
    "docs = load_markdown_file()\n",
    "\n",
    "# Filtering to only include page_content and source metadata.\n",
    "minimal_docs = filter_to_minimal_docs(docs)\n",
    "\n",
    "# Split the minimized documents into text chunks\n",
    "text_chunks = text_split(minimal_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac214c1b",
   "metadata": {},
   "source": [
    "## Performing the Vector Embedding on Text Data:\n",
    "\n",
    "The Embedding Model is **Sentence-Transformers**\n",
    "\n",
    "1. The chunks are processed and converted to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b36ba7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:01<00:00, 86.18it/s, Materializing param=pooler.dense.weight]                              \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embeddingModel = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa7d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_index = len(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73f32a5",
   "metadata": {},
   "source": [
    "### Storing the vector embeddings for each text chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03dc0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorEmbeddings = []\n",
    "\n",
    "for id, chunk in enumerate(text_chunks):\n",
    "    source = chunk.metadata['source']\n",
    "    text = chunk.page_content\n",
    "    embedding = embeddingModel.encode(text).tolist()\n",
    "\n",
    "    data = {\n",
    "        \"id\": id + 1,\n",
    "        \"vector\": embedding,\n",
    "        \"meta\": {\n",
    "            \"source\": source,\n",
    "            \"text\": text\n",
    "        }\n",
    "    }\n",
    "    vectorEmbeddings.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b60763b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorEmbeddings[0][\"vector\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073b9d2b",
   "metadata": {},
   "source": [
    "### Setting up for connecting with Endee API powered by Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64c2d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Index name for Endee Vector Database\n",
    "INDEX_NAME = \"enterprise_knowledge_base\"\n",
    "\n",
    "# URL for Endee API service\n",
    "ENDEE_URL = \"http://127.0.0.1:8000\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f22fb",
   "metadata": {},
   "source": [
    "### Creating a Index in Endee Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac1eea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payload for creating an index in Endee Vector DB\n",
    "payload_for_create_index = {\n",
    "    \"index_name\": INDEX_NAME,\n",
    "    \"dimension\": len(vectorEmbeddings[0][\"vector\"]),\n",
    "    \"precision\": \"INT16D\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9246e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message for index creation: {'index_name': 'enterprise_knowledge_base', 'status': 'index created'}\n"
     ]
    }
   ],
   "source": [
    "response_for_create_index = requests.post(\n",
    "    f\"{ENDEE_URL}/index/create\",\n",
    "    json=payload_for_create_index\n",
    ")\n",
    "print(f\"Message for index creation: {response_for_create_index.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173a26a2",
   "metadata": {},
   "source": [
    "### Checking for the Existence of the Index in the Endee Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f50c71d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index_name': 'enterprise_knowledge_base', 'status': 'index loaded'}\n"
     ]
    }
   ],
   "source": [
    "response_for_get_index = requests.post(\n",
    "    f\"{ENDEE_URL}/index/get\",\n",
    "    json={\n",
    "        \"index_name\": INDEX_NAME\n",
    "    })\n",
    "print(response_for_get_index.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8770f",
   "metadata": {},
   "source": [
    "### Inserting the Embedded Vectors into Endee Vector DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "785c62ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_to_insert_multiple_data = {\n",
    "    \"index_name\": INDEX_NAME,\n",
    "    \"embedded_vectors\": vectorEmbeddings\n",
    "}\n",
    "\n",
    "payload_to_insert_single_data = {\n",
    "    \"index_name\": INDEX_NAME,\n",
    "    \"embedded_vectors\": [vectorEmbeddings[0]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3f8db8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 413, 'status': 'vectors upserted'}\n"
     ]
    }
   ],
   "source": [
    "response_for_multiple_insert = requests.post(\n",
    "    f\"{ENDEE_URL}/index/upsert\",\n",
    "    json=payload_to_insert_multiple_data\n",
    ")\n",
    "\n",
    "# response_for_single_insert = requests.post(\n",
    "#     f\"{ENDEE_URL}/index/upsert\",\n",
    "#     json=payload_to_insert_single_data\n",
    "# )\n",
    "\n",
    "print(response_for_multiple_insert.json())\n",
    "# print(response_for_single_insert.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30dd914",
   "metadata": {},
   "source": [
    "### Retrieving the Top K most relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d460756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to become a developer at Gitlab?\"\n",
    "embedding_for_query = embeddingModel.encode(query).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8057467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"index_name\": INDEX_NAME,\n",
    "    \"vector\": embedding_for_query,\n",
    "    \"top_k\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a2d90789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sends a query to the Endee API\n",
    "response = requests.post(\n",
    "    f\"{ENDEE_URL}/index/query\",\n",
    "    json=payload\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9a7fd268",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853cf05e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
