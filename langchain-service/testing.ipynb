{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661842c6",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "1. Load the **PDFs or Markdown** files in Order\n",
    "2. Extract the **data/content** from the PDF\n",
    "3. Then perform chunking - *Because the context window for the LLMs are small*\n",
    "4. Then pass it to the **Embedding Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23e8ac08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sayan\\OneDrive\\Desktop\\internship-project\\enterprise-knowledge-copilot\\langchain-service\\.langchain-venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 1. Loading of the PDFs\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e85ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "FILE_PATH = \"C:/Users/sayan/OneDrive/Desktop/internship-project/enterprise-knowledge-copilot/data\"\n",
    "\n",
    "# Function to load PDF files from a specified directory\n",
    "def load_pdf_file(file_path=FILE_PATH):\n",
    "    if not os.path.isdir(file_path):\n",
    "        print(f\"Error: The provided path '{file_path}' is not a directory.\")\n",
    "        return []\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "        file_path,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader,\n",
    "        recursive=True,\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Function to load Markdown files from a specified directory\n",
    "# This function doesn't extract all the required data.\n",
    "def load_markdown_file(file_path=FILE_PATH):\n",
    "    if not os.path.isdir(file_path):\n",
    "        print(f\"Error: The provided path '{file_path}' is not a directory.\")\n",
    "        return []\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "        file_path,\n",
    "        glob=\"**/*.md\",           \n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs={\"encoding\": \"utf-8\"}\n",
    "    )\n",
    "\n",
    "    docs = loader.load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84b72cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to clean markdown text\n",
    "def clean_markdown(md_text: str) -> str:\n",
    "    # 1. Remove YAML front matter\n",
    "    md_text = re.sub(r\"^---.*?---\", \"\", md_text, flags=re.DOTALL)\n",
    "\n",
    "    # 2. Convert markdown → HTML\n",
    "    html = markdown.markdown(md_text)\n",
    "\n",
    "    # 3. Parse HTML\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # 4. Remove unwanted tags\n",
    "    for tag in soup([\"script\", \"style\", \"iframe\", \"img\", \"table\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # 5. Get text\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "    # 6. Remove markdown links but keep text\n",
    "    text = re.sub(r\"\\[(.*?)\\]\\(.*?\\)\", r\"\\1\", text)\n",
    "\n",
    "    # 7. Normalize whitespace\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "459975d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters documents to only include page_content and source metadata.\n",
    "def filter_to_minimal_docs(docs):\n",
    "    minimal_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        full_path = doc.metadata.get(\"source\", \"\")\n",
    "        file_name = os.path.basename(full_path)\n",
    "\n",
    "        minimal_doc = Document(\n",
    "            page_content=doc.page_content,\n",
    "            metadata={\n",
    "                \"title\": doc.metadata.get(\"title\"),\n",
    "                \"description\": doc.metadata.get(\"description\"),\n",
    "                \"source\": file_name\n",
    "            }\n",
    "        )\n",
    "\n",
    "        minimal_docs.append(minimal_doc)\n",
    "\n",
    "    return minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b0c5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into smaller chunks\n",
    "def text_split(minimal_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(minimal_docs)\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2318f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import frontmatter\n",
    "from pathlib import Path\n",
    "\n",
    "# This will be the Function to load MARKDOWN FILES\n",
    "def load_md_with_metadata(path):\n",
    "    docs = []\n",
    "\n",
    "    for file in Path(path).rglob(\"*.md\"):\n",
    "        post = frontmatter.load(file)\n",
    "\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=post.content,\n",
    "                metadata={\n",
    "                    \"title\": post.get(\"title\"),\n",
    "                    \"description\": post.get(\"description\"),\n",
    "                    \"source\": str(file)\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62cb2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Markdown files\n",
    "docs = load_md_with_metadata(FILE_PATH)\n",
    "\n",
    "# Filtering to only include page_content and source metadata.\n",
    "minimal_docs = filter_to_minimal_docs(docs)\n",
    "\n",
    "# Split the minimized documents into text chunks\n",
    "text_chunks = text_split(minimal_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac214c1b",
   "metadata": {},
   "source": [
    "## Performing the Vector Embedding on Text Data:\n",
    "\n",
    "The Embedding Model is **Sentence-Transformers**\n",
    "\n",
    "1. The chunks are processed and converted to vectors\n",
    "2. Then the embedded vectors is then sent to the Endee Vector Database using the Endee API\n",
    "3. Once the Data is stored in the the Vector DB, it is now ready to serve as a Knowledge base for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b36ba7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 131.34it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embeddingModel = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa7d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_index = len(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73f32a5",
   "metadata": {},
   "source": [
    "### Storing the vector embeddings for each text chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "03dc0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorEmbeddings = []\n",
    "\n",
    "for id, chunk in enumerate(text_chunks):\n",
    "    source = chunk.metadata.get('source', \"\")\n",
    "    title = chunk.metadata.get(\"title\", \"\")\n",
    "    description = chunk.metadata.get(\"description\", \"\")\n",
    "    text = chunk.page_content\n",
    "    embedding = embeddingModel.encode(text).tolist()\n",
    "\n",
    "    data = {\n",
    "        \"id\": id + 1,\n",
    "        \"vector\": embedding,\n",
    "        \"meta\": {\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"source\": source,\n",
    "            \"text\": text\n",
    "        }\n",
    "    }\n",
    "    vectorEmbeddings.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b60763b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'vector': [-0.07971559464931488, -0.04249269515275955, 0.034429214894771576, -0.004940586164593697, 0.00379744078963995, -0.11942380666732788, 0.03763573616743088, -0.032276514917612076, 0.03974507749080658, -0.08302801847457886, -0.07007479667663574, 0.07063862681388855, 0.08201108127832413, -0.004002273548394442, 0.04556172341108322, -0.05214153602719307, 0.01700224168598652, 0.04116419702768326, 0.03911973908543587, -0.08725880831480026, -0.0489850677549839, 0.011393359862267971, 0.023980718106031418, 0.03097580559551716, 0.022394699975848198, -0.06662602722644806, 0.003506705164909363, -0.016434794291853905, -0.07027978450059891, -0.09300784021615982, -0.035802412778139114, 0.015175309032201767, 0.017852244898676872, 0.072708860039711, -0.10882662236690521, 0.15073974430561066, -0.011714130640029907, -0.026711083948612213, -0.07094957679510117, -0.021012822166085243, -0.046953000128269196, -0.05267389491200447, -0.02817157655954361, -0.01358864177018404, 0.003454051911830902, -0.05860603228211403, 0.01951783150434494, -0.09046372026205063, -0.05343179777264595, 0.024684222415089607, 0.027680179104208946, -0.11612118035554886, 0.04124017432332039, 0.06280548125505447, 0.03648786246776581, 0.025826120749115944, -0.017751483246684074, -0.02786664292216301, -0.022168120369315147, 0.05908433347940445, 0.00885429885238409, 0.024431008845567703, -0.08242829889059067, -0.0021092642564326525, -0.0043621910735964775, -0.06317584216594696, -0.03909127414226532, 0.041001707315444946, -0.03733230009675026, -0.22170808911323547, 0.05072906240820885, -0.06516119837760925, -0.02944880537688732, -0.04542173817753792, 0.036400601267814636, 0.0014578189002349973, 0.033511076122522354, 0.04515329748392105, 0.08280055224895477, -0.08246783912181854, 0.04306977242231369, -0.03850150480866432, -0.06068752333521843, 0.07174091786146164, -0.043511006981134415, 0.04230484366416931, 0.031144948676228523, 0.050428394228219986, 0.014270772226154804, 0.0715472623705864, 0.039065171033144, 0.025646507740020752, 0.05074990540742874, 0.008723515085875988, 0.007307341322302818, 0.01090551819652319, 0.02867361158132553, -0.022937040776014328, -0.01944589987397194, 0.04285707324743271, -0.021575206890702248, -0.046347491443157196, 0.0822012647986412, 0.03939834237098694, -0.06777450442314148, -0.01720982789993286, 0.030460868030786514, 0.01527333166450262, 0.054042018949985504, -0.023562418296933174, -0.008418277837336063, -0.011290197260677814, -0.02564455196261406, 0.02231261134147644, -0.020120413973927498, 0.0005738127510994673, -0.05007634684443474, 0.036900293081998825, 0.057102497667074203, -0.14160925149917603, -0.03833456709980965, 0.12279048562049866, 0.05113130807876587, -0.03363708779215813, 0.09341293573379517, 0.019544074311852455, -0.02242317982017994, 6.261486013893405e-34, 0.003854654962196946, 0.06770750880241394, 0.015630511566996574, 0.06907855719327927, -0.015836939215660095, 0.020764030516147614, -0.06030481681227684, -0.034401241689920425, -0.07133462280035019, -0.029707398265600204, -0.06373623758554459, 0.11533941328525543, -0.01255796104669571, 0.08302661776542664, 0.0545881949365139, -0.033377259969711304, -0.02269889973104, 0.04322607070207596, 0.01874677464365959, -0.016520921140909195, -0.04597390443086624, 0.003687677439302206, -0.06839204579591751, 0.1333009898662567, 0.060406070202589035, -0.01439431868493557, 0.02603285200893879, 0.013689391314983368, 0.018434572964906693, 0.051433347165584564, 0.01990286447107792, 0.03619254752993584, 0.03252100571990013, -0.01628401130437851, -0.03806843236088753, 0.043883420526981354, -0.10267319530248642, -0.04013034701347351, -0.0003141248016618192, 0.029057221487164497, 0.00825683306902647, -0.0240118857473135, -0.035331595689058304, 0.02027260698378086, -0.029257526621222496, -0.07322456687688828, 0.021835841238498688, 0.051445357501506805, 0.14605195820331573, 0.07607055455446243, -0.04308328405022621, 0.020521050319075584, 0.11063732206821442, 0.0046476623974740505, 0.05893557891249657, 0.060911599546670914, 0.06525608897209167, -0.014447757042944431, 0.019840024411678314, 0.02296467125415802, -0.0384976789355278, 0.05013403296470642, -0.05991515517234802, -0.028093205764889717, 0.0074710180051624775, 0.022871293127536774, 0.021880457177758217, 0.003916274756193161, 0.0762181431055069, 0.02223069593310356, -0.07297537475824356, 0.04490148276090622, 0.04339887574315071, 0.033471230417490005, -0.06873691827058792, -0.01394882332533598, -0.08281204849481583, 0.07324191927909851, 0.027551451697945595, 0.0421968549489975, -0.04126695916056633, -0.00047623502905480564, 0.03065069578588009, 0.04582564905285835, 0.052755627781152725, 0.07411681860685349, 0.01797618344426155, -0.016852600499987602, 0.00221250974573195, 0.11711466312408447, 0.01793673448264599, -0.025025278329849243, -0.02891489490866661, 0.07312288135290146, 0.04342060536146164, -2.094842421457652e-33, -0.03837927430868149, -0.021058756858110428, -0.08304346352815628, 0.011144292540848255, 0.0434323288500309, -0.09390244632959366, 0.05127307400107384, -0.062164612114429474, 0.020578239113092422, 0.04968271777033806, 0.0327623225748539, -0.00015793954662512988, -0.03854232653975487, -0.0017428192077204585, -0.015530702657997608, -0.04284664988517761, -0.028892721980810165, -0.05232950672507286, -0.05578015372157097, 0.016121068969368935, -0.030313542112708092, -0.020975669845938683, 0.042376477271318436, 0.04876992478966713, -0.00806322880089283, -0.0220325980335474, 0.03468784689903259, 0.03492284566164017, -0.036922335624694824, -0.08360550552606583, 0.04371799901127815, -0.07023772597312927, 0.009544918313622475, 0.016852784901857376, -0.035678453743457794, -0.018808353692293167, -0.06991670280694962, 0.052082620561122894, 0.005928490310907364, -0.10913792997598648, 0.04644026979804039, -0.059516530483961105, 0.0042638094164431095, -0.05558788403868675, -0.03730940818786621, 0.07108329236507416, 0.011372804641723633, -0.05111375451087952, -0.08882080018520355, -0.07387493550777435, 0.020453236997127533, 0.05783538520336151, 0.053846534341573715, -0.10731062293052673, 0.04304106906056404, -0.04959016293287277, 0.05116022750735283, 0.028796479105949402, 0.007217561826109886, -0.03746050223708153, 0.03263728693127632, 0.019837800413370132, 0.11611700803041458, 0.09488752484321594, -0.05186592787504196, -0.06263019889593124, 0.0022649827878922224, -0.005557715427130461, -0.0805453434586525, -0.02150626666843891, 0.026993922889232635, -0.038495372980833054, -0.03752075508236885, -0.0483902208507061, -0.015176074579358101, -0.018882768228650093, -0.09924980998039246, -0.01952514797449112, -0.010181614197790623, -0.04212070256471634, 0.03040708787739277, 0.012820828706026077, 0.013025055639445782, 0.03787393122911453, 0.004995136056095362, 0.0058286297135055065, -0.019817432388663292, 0.06013210117816925, 0.06314659118652344, -0.028175093233585358, -0.06410401314496994, -0.015012562274932861, -0.07337065041065216, -0.028134195134043694, 0.05433476343750954, -4.008936471677771e-08, -0.03605515882372856, 0.052639495581388474, -0.060602445155382156, -0.056178830564022064, -0.03886309638619423, 0.021349279209971428, 0.00012531373067758977, 0.029021652415394783, -0.0001340387825621292, 0.0514901727437973, -0.016460243612527847, -0.020086131989955902, -0.00935575645416975, 0.035565804690122604, 0.08290944248437881, 0.031037814915180206, 0.004975385032594204, 0.05041227862238884, -0.028835810720920563, -0.1331600397825241, -0.061159491539001465, 0.06054599583148956, -0.0351775661110878, 0.0007778522558510303, 0.004165795166045427, 0.021525751799345016, -0.009872864000499249, 0.03477583825588226, 0.009720548056066036, -0.06556305289268494, 0.07150144875049591, -0.0040063755586743355, -0.04983251914381981, -0.030452251434326172, 0.0628860741853714, -0.006915051490068436, -0.02378094010055065, 0.026104629039764404, 0.07292372733354568, 0.10461526364088058, -0.054015450179576874, -0.01173953153192997, 0.026059739291667938, 0.031888071447610855, -0.07165390998125076, 0.007306771818548441, -0.019812431186437607, -0.04432341456413269, -0.017740121111273766, -0.020302217453718185, -0.005952811799943447, -0.014173034578561783, -0.04805275425314903, 0.00960471760481596, -0.006709568202495575, 0.029827991500496864, 0.052846867591142654, -0.06733302772045135, -0.0030085239559412003, 0.012768740765750408, 0.02554268203675747, 0.0025134002789855003, 0.1129646897315979, 0.006027577444911003], 'meta': {'title': 'Backend Engineering', 'description': 'Backend Engineers at GitLab work on our product. This includes both the open source and enterprise editions, and the GitLab.com service.', 'source': 'backend-engineer.md', 'text': '## Backend Engineering Roles at GitLab\\n\\nBackend Engineers at GitLab work on our product. This includes both the open source version of GitLab, the enterprise editions, and the GitLab.com service as well. They work with peers on teams dedicated to areas of the product. They work together with product managers, designers, and [frontend engineers](/job-families/engineering/development/frontend/) to solve common goals.'}}\n"
     ]
    }
   ],
   "source": [
    "print(vectorEmbeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073b9d2b",
   "metadata": {},
   "source": [
    "### Setting up for connecting with Endee API powered by Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64c2d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Index name for Endee Vector Database\n",
    "INDEX_NAME = \"enterprise_knowledge_base\"\n",
    "\n",
    "# URL for Endee API service\n",
    "ENDEE_URL = \"http://127.0.0.1:8000\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f22fb",
   "metadata": {},
   "source": [
    "### Creating a Index in Endee Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac1eea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payload for creating an index in Endee Vector DB\n",
    "payload_for_create_index = {\n",
    "    \"index_name\": INDEX_NAME,\n",
    "    \"dimension\": len(vectorEmbeddings[0][\"vector\"]),\n",
    "    \"precision\": \"INT16D\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9246e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message for index creation: {'index_name': 'enterprise_knowledge_base', 'status': 'index created'}\n"
     ]
    }
   ],
   "source": [
    "response_for_create_index = requests.post(\n",
    "    f\"{ENDEE_URL}/index/create\",\n",
    "    json=payload_for_create_index\n",
    ")\n",
    "print(f\"Message for index creation: {response_for_create_index.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173a26a2",
   "metadata": {},
   "source": [
    "### Checking for the Existence of the Index in the Endee Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f50c71d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index_name': 'enterprise_knowledge_base', 'status': 'index loaded'}\n"
     ]
    }
   ],
   "source": [
    "response_for_get_index = requests.post(\n",
    "    f\"{ENDEE_URL}/index/get\",\n",
    "    json={\n",
    "        \"index_name\": INDEX_NAME\n",
    "    })\n",
    "print(response_for_get_index.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8770f",
   "metadata": {},
   "source": [
    "### Inserting the Embedded Vectors into Endee Vector DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c62ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_to_insert_multiple_data = {\n",
    "    \"index_name\": INDEX_NAME,\n",
    "    \"embedded_vectors\": vectorEmbeddings\n",
    "}\n",
    "\n",
    "payload_to_insert_single_data = {\n",
    "    \"index_name\": INDEX_NAME,\n",
    "    \"embedded_vectors\": [vectorEmbeddings[0]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f8db8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Cannot insert more than 1000 vectors at a time'}\n"
     ]
    }
   ],
   "source": [
    "response_for_multiple_insert = requests.post(\n",
    "    f\"{ENDEE_URL}/index/upsert\",\n",
    "    json=payload_to_insert_multiple_data\n",
    ")\n",
    "\n",
    "# response_for_single_insert = requests.post(\n",
    "#     f\"{ENDEE_URL}/index/upsert\",\n",
    "#     json=payload_to_insert_single_data\n",
    "# )\n",
    "\n",
    "print(response_for_multiple_insert.json())\n",
    "# print(response_for_single_insert.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f87a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 805, 'status': 'vectors upserted'}\n"
     ]
    }
   ],
   "source": [
    "# Inserting the Data:\n",
    "\n",
    "payload = {\n",
    "    \"index_name\": INDEX_NAME,\n",
    "    \"embedded_vectors\": vectorEmbeddings[1740:]\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{ENDEE_URL}/index/upsert\",\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30dd914",
   "metadata": {},
   "source": [
    "### Retrieving the Top K most relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d460756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is GitLab?\"\n",
    "embedding_for_query = embeddingModel.encode(query).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8057467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"index_name\": INDEX_NAME,\n",
    "    \"vector\": embedding_for_query,\n",
    "    \"top_k\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a2d90789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sends a query to the Endee API\n",
    "response = requests.post(\n",
    "    f\"{ENDEE_URL}/index/query\",\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c91f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"results\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce9d16f",
   "metadata": {},
   "source": [
    "### Build complete RAG Pipeline:\n",
    "\n",
    "1. Using GROQ Cloud for it's Free APIs and Fast response\n",
    "2. Using Langchain as a warapper\n",
    "\n",
    "### How  does this work:\n",
    "\n",
    "1. `retriever`: returns `List[Document]` for a query (Find relevant information)\n",
    "2. `prompt`: a prompt template with `{context}` and `{input}` (Frame the question for the AI)\n",
    "3. `chatModel`: an LLM runnable (Generate the answer)\n",
    "4. `RunnablePassthrough`: Carry the user’s question forward unchanged\n",
    "5. `StrOutputParser`: Clean the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3b93d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the environment variables from the .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "853cf05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "142727ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a professional and a helpful assistant for a Company named GitLab.\n",
    "Answer in three to five sentences for general questions.\n",
    "and if the answer needs to be more elaborate, provide more details like provide step by step instructions only when needed.\n",
    "Use the context given below for your reference and keep the answer concise and to the point.\n",
    "Respond in detail only when the user specifies it.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "If a user ask whether they can upload a Document, respond with \"Yes, you can upload a PDF Document only.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ac4594bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "98f5daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is wrapper for retrieving the data\n",
    "def endee_retriever(query: str):\n",
    "    embedding_for_query = embeddingModel.encode(query).tolist()\n",
    "\n",
    "    payload = {\n",
    "        \"index_name\": INDEX_NAME,\n",
    "        \"vector\": embedding_for_query,\n",
    "        \"top_k\": 10\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"{ENDEE_URL}/index/query\",\n",
    "        json=payload\n",
    "    )\n",
    "\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    docs = []\n",
    "    for d in data.get(\"matches\", []):\n",
    "        text = d.get(\"text\", \"\")\n",
    "\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"similarity\": d.get(\"similarity\"),\n",
    "                    \"source\": d.get(\"source\"),\n",
    "                    \"title\": d.get(\"title\"),\n",
    "                    \"description\": d.get(\"description\")\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return docs\n",
    "\n",
    "retriever = RunnableLambda(endee_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e4271d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f93560e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rag pipeline:\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | (lambda docs: \"\\n\\n\".join(d.page_content for d in docs)),\n",
    "        \"input\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a6c815af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To prepare for an interview, you should learn about the company, the role you're applying for, and the required skills. Review the job description and requirements, and be ready to provide examples of your experience and skills. You should also practice common interview questions and be prepared to ask questions to the interviewer. Additionally, learn about GitLab's products and services, such as Git version control, CI/CD pipelines, and Agile project planning.\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample test:\n",
    "rag_chain.invoke(\"If I want to prepare for an Interview, what all should I need to learn, can you guide me in this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cf8ce6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To prepare for an interview, here's a step-by-step guide:\n",
      "\n",
      "1. **Research the Company**: Learn about GitLab's products, mission, values, and culture to understand our vision and goals.\n",
      "2. **Review the Job Description**: Study the job requirements, responsibilities, and skills needed for the position you're applying for.\n",
      "3. **Update Your Resume and Online Profiles**: Ensure your resume, LinkedIn profile, and other social media platforms are up-to-date and highlight your relevant skills and experiences.\n",
      "4. **Practice Common Interview Questions**: Prepare answers to common interview questions, such as \"Why do you want to work at GitLab?\" or \"What are your strengths and weaknesses?\"\n",
      "5. **Develop Your Technical Skills**: If you're applying for a technical role, review and practice your coding skills, and be prepared to answer technical questions related to your field.\n",
      "6. **Prepare Questions to Ask**: Come up with a list of questions to ask the interviewer, such as \"What are the biggest challenges facing the team right now?\" or \"Can you tell me more about the company culture?\"\n",
      "7. **Mock Interviews**: Practice your interview skills with a friend or family member, or use online resources to simulate a mock interview.\n",
      "8. **Learn About Our Products and Services**: Familiarize yourself with GitLab's products and services, such as GitLab CI/CD, GitLab Runner, and GitLab Pages.\n",
      "9. **Understand Our Values and Culture**: Learn about GitLab's values, such as collaboration, transparency, and results, and be prepared to give examples of how you embody these values.\n",
      "10. **Get Ready Logistically**: Make sure you have a quiet, reliable space for the interview, and test your audio and video equipment beforehand.\n",
      "\n",
      "By following these steps, you'll be well-prepared for your interview and can showcase your skills and experiences to the best of your ability.\n"
     ]
    }
   ],
   "source": [
    "# Sample test:\n",
    "print(rag_chain.invoke(\"If I want to prepare for an Interview, what all should I need to learn, can you guide me in this. Tell me step by step in detail\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e2e8c206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am a professional and helpful assistant for GitLab, a company that provides a platform for version control and collaborative software development. My role is to assist users with their queries and provide support. I can help with general questions, provide step-by-step instructions when needed, and offer guidance on using GitLab's features and services. I'm here to help with any questions you may have.\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Tell me about yourself\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "35c30277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: You can check the company's website for employee referral programs or ask current employees if they can refer you. You can also utilize professional networking platforms like LinkedIn to connect with current or former employees and ask for referrals. Additionally, you can reach out to the company's HR department to inquire about their referral process.\n",
      "Assistant: The company has various job roles, but the specific roles are not mentioned in the given context. If you need more information, please provide more context or clarify which company you are referring to. However, some common job roles in general include administrative, technical, marketing, and sales positions. For more details, please provide additional context.\n",
      "Assistant: You can check the available openings on the official GitLab website, specifically on their careers or jobs page. They list various positions, including engineering, marketing, sales, and more. For the most up-to-date information, I recommend visiting their website directly.\n",
      "Assistant: I am a general assistant, but my specific company affiliation is not provided.\n",
      "Exiting the chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Testing the RAG pipeline with user input in a loop\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting the chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    response = rag_chain.invoke(user_input)\n",
    "    print(f\"Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0edd47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
