{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661842c6",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "1. Load the **PDFs or Markdown** files in Order\n",
    "2. Extract the **data/content** from the PDF\n",
    "3. Then perform chunking - *Because the context window for the LLMs are small*\n",
    "4. Then pass it to the **Embedding Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e8ac08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sayan\\OneDrive\\Desktop\\internship-project\\enterprise-knowledge-copilot\\langchain-service\\.langchain-venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 1. Loading of the PDFs\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5b6f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "DATA_FILE_PATH = os.path.join(parent_dir, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e85ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load PDF files from a specified directory\n",
    "def load_pdf_file(file_path=DATA_FILE_PATH):\n",
    "    if not os.path.isdir(file_path):\n",
    "        print(f\"Error: The provided path '{file_path}' is not a directory.\")\n",
    "        return []\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "        file_path,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader,\n",
    "        recursive=True,\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Function to load Markdown files from a specified directory\n",
    "# This function doesn't extract all the required data.\n",
    "def load_markdown_file(file_path=DATA_FILE_PATH):\n",
    "    if not os.path.isdir(file_path):\n",
    "        print(f\"Error: The provided path '{file_path}' is not a directory.\")\n",
    "        return []\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "        file_path,\n",
    "        glob=\"**/*.md\",           \n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs={\"encoding\": \"utf-8\"}\n",
    "    )\n",
    "\n",
    "    docs = loader.load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2318f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import frontmatter\n",
    "from pathlib import Path\n",
    "\n",
    "# This will be the Function to load MARKDOWN FILES\n",
    "def load_md_with_metadata(path):\n",
    "    docs = []\n",
    "\n",
    "    for file in Path(path).rglob(\"*.md\"):\n",
    "        post = frontmatter.load(file)\n",
    "\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=post.content,\n",
    "                metadata={\n",
    "                    \"title\": post.get(\"title\"),\n",
    "                    \"description\": post.get(\"description\"),\n",
    "                    \"source\": str(file)\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84b72cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to clean markdown text\n",
    "def clean_markdown(md_text: str) -> str:\n",
    "    # 1. Remove YAML front matter\n",
    "    md_text = re.sub(r\"^---.*?---\", \"\", md_text, flags=re.DOTALL)\n",
    "\n",
    "    # 2. Convert markdown â†’ HTML\n",
    "    html = markdown.markdown(md_text)\n",
    "\n",
    "    # 3. Parse HTML\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # 4. Remove unwanted tags\n",
    "    for tag in soup([\"script\", \"style\", \"iframe\", \"img\", \"table\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # 5. Get text\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "    # 6. Remove markdown links but keep text\n",
    "    text = re.sub(r\"\\[(.*?)\\]\\(.*?\\)\", r\"\\1\", text)\n",
    "\n",
    "    # 7. Normalize whitespace\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459975d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters documents to only include page_content and source metadata.\n",
    "def filter_to_minimal_docs(docs):\n",
    "    minimal_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        full_path = doc.metadata.get(\"source\", \"\")\n",
    "        file_name = os.path.basename(full_path)\n",
    "\n",
    "        minimal_doc = Document(\n",
    "            page_content=doc.page_content,\n",
    "            metadata={\n",
    "                \"title\": doc.metadata.get(\"title\"),\n",
    "                \"description\": doc.metadata.get(\"description\"),\n",
    "                \"source\": file_name\n",
    "            }\n",
    "        )\n",
    "\n",
    "        minimal_docs.append(minimal_doc)\n",
    "\n",
    "    return minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b0c5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into smaller chunks\n",
    "def text_split(minimal_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(minimal_docs)\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62cb2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Markdown files\n",
    "docs = load_md_with_metadata(DATA_FILE_PATH)\n",
    "\n",
    "# Filtering to only include page_content and source metadata.\n",
    "minimal_docs = filter_to_minimal_docs(docs)\n",
    "\n",
    "# Split the minimized documents into text chunks\n",
    "text_chunks = text_split(minimal_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac214c1b",
   "metadata": {},
   "source": [
    "## Performing the Vector Embedding on Text Data:\n",
    "\n",
    "The Embedding Model is **Sentence-Transformers**\n",
    "\n",
    "1. The chunks are processed and converted to vectors\n",
    "2. Then the embedded vectors is then sent to the Endee Vector Database using the Endee API\n",
    "3. Once the Data is stored in the the Vector DB, it is now ready to serve as a Knowledge base for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b36ba7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 110.97it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embeddingModel = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73f32a5",
   "metadata": {},
   "source": [
    "### Storing the vector embeddings for each text chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03dc0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 2949\n",
    "vectorEmbeddings = []\n",
    "\n",
    "for id, chunk in enumerate(text_chunks):\n",
    "    source = chunk.metadata.get('source', \"\")\n",
    "    title = chunk.metadata.get(\"title\", \"\")\n",
    "    description = chunk.metadata.get(\"description\", \"\")\n",
    "    text = chunk.page_content\n",
    "    embedding = embeddingModel.encode(text).tolist()\n",
    "\n",
    "    data = {\n",
    "        \"id\": id + start_idx,\n",
    "        \"vector\": embedding,\n",
    "        \"meta\": {\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"source\": source,\n",
    "            \"text\": text\n",
    "        }\n",
    "    }\n",
    "    vectorEmbeddings.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073b9d2b",
   "metadata": {},
   "source": [
    "### Setting up for connecting with Endee API powered by Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64c2d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import ConnectionError, Timeout, HTTPError\n",
    "\n",
    "# Index name for Endee Vector Database\n",
    "INDEX_NAME = \"enterprise_knowledge_base\"\n",
    "\n",
    "# URL for Endee API service\n",
    "ENDEE_URL = \"http://127.0.0.1:8000\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f22fb",
   "metadata": {},
   "source": [
    "### Creating a Index in Endee Vector Database\n",
    "\n",
    "Using only the Sparse Dimensions (***Vector Embeddings***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1eea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payload for creating an index in Endee Vector DB\n",
    "payload = {\n",
    "    \"index_name\": INDEX_NAME,\n",
    "    \"dimension\": embeddingModel.get_sentence_embedding_dimension(),\n",
    "    \"precision\": \"INT16D\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9246e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP error, Code: 500, Message: Conflict: Index with this name already exists for this user\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    response = requests.post(\n",
    "        f\"{ENDEE_URL}/index/create\",\n",
    "        json=payload\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    print(f\"Message for index creation: {response.json()}\")\n",
    "\n",
    "except ConnectionError:\n",
    "    print(\"Backend service is not reachable (is it running?)\")\n",
    "    \n",
    "except Timeout:\n",
    "    print(\"Request timed out\")\n",
    "    \n",
    "except HTTPError as e:\n",
    "    try:\n",
    "        err = e.response.json()\n",
    "        print(\"Error message:\", err.get(\"error\"))\n",
    "    except ValueError:\n",
    "        print(\"Raw error:\", e.response.text)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173a26a2",
   "metadata": {},
   "source": [
    "### Checking for the Existence of the Index in the Endee Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50c71d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index_name': 'enterprise_knowledge_base', 'status': 'index loaded'}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = requests.post(\n",
    "        f\"{ENDEE_URL}/index/get\",\n",
    "        json={\n",
    "            \"index_name\": INDEX_NAME\n",
    "        })\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    print(response.json())\n",
    "    \n",
    "except ConnectionError:\n",
    "    print(\"Backend service is not reachable (is it running?)\")\n",
    "    \n",
    "except Timeout:\n",
    "    print(\"Request timed out\")\n",
    "    \n",
    "except HTTPError as e:\n",
    "    try:\n",
    "        err = e.response.json()\n",
    "        print(\"Error message:\", err.get(\"error\"))\n",
    "    except ValueError:\n",
    "        print(\"Raw error:\", e.response.text)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8770f",
   "metadata": {},
   "source": [
    "### Inserting the Embedded Vectors into Endee Vector DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa23f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Manav Khurana is a business-minded technical person. Two things drive my work practice:\n",
    "Solving customer problems â€“ more of them and doing so in the best way.\n",
    "\n",
    "Optimizing for business results in what we do.\n",
    "\n",
    "I take a data-driven approach to make sure weâ€™re making progress in both. Youâ€™ll often see me mapping inputs to outputs with data, putting ourselves in our customersâ€™ shoes, and asking about customer feedback. I get a lot of satisfaction in going about that work with my colleagues, building life-long friendships along the way. I believe everything else â€“ feeling of impact, fun, recognition, money, etc â€“ follows.\n",
    "I was born and raised in New Delhi, India. I came to the U.S. to study electrical and computer engineering in New York and then moved to San Francisco for whatâ€™s now been a 25-year career in tech. My wife, whom I met in college, is a psychologist and keeps me accountable to our value system ðŸ™‚. We have a 12-year-old son, who is pure joy to be around and inspires us with his curiosity. We cherish our time together at home, with our community, and in our travels. In my spare time, I like sweating it out on a tennis court a couple of times a week.\n",
    "I respond to all pronunciations of my name but prefer the original â€“ Maah-nuhv.\n",
    "\"\"\"\n",
    "\n",
    "title = \"Manav Khurana - Chief Product and Marketing Officer - README\"\n",
    "description = \"Learn more about working with Manav Khurana, CPMO at GitLab.\"\n",
    "\n",
    "vectors = embeddingModel.encode(text).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c62ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"index_name\": INDEX_NAME,\n",
    "    \"embedded_vectors\": [vectors]\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(\n",
    "        f\"{ENDEE_URL}/index/upsert\",\n",
    "        json=payload\n",
    "    )\n",
    "\n",
    "    response.raise_for_status()\n",
    "    print(response.json())\n",
    "    \n",
    "except ConnectionError:\n",
    "    print(\"Backend service is not reachable (is it running?)\")\n",
    "    \n",
    "except Timeout:\n",
    "    print(\"Request timed out\")\n",
    "    \n",
    "except HTTPError as e:\n",
    "    try:\n",
    "        err = e.response.json()\n",
    "        print(\"Error message:\", err.get(\"error\"))\n",
    "    except ValueError:\n",
    "        print(\"Raw error:\", e.response.text)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94cc98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing list slicing because, insertion limit is 1000 vectors, and I am keeping 950 vectors per upsert\n",
    "slices = []\n",
    "\n",
    "end = len(vectorEmbeddings) // 950\n",
    "prev = 0\n",
    "for i in range(1, end + 1):\n",
    "    slices.append((prev, 950 * i + 1))\n",
    "    prev = 950 * i + 1\n",
    "slices.append((prev, len(vectorEmbeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bf408f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 951, 'status': 'vectors upserted'}\n",
      "{'count': 950, 'status': 'vectors upserted'}\n",
      "{'count': 950, 'status': 'vectors upserted'}\n",
      "{'count': 950, 'status': 'vectors upserted'}\n",
      "{'count': 843, 'status': 'vectors upserted'}\n"
     ]
    }
   ],
   "source": [
    "for start, end in slices:\n",
    "    vectors = vectorEmbeddings[start:end]\n",
    "    \n",
    "    if not upsertVectors(vectors):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8db8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to UPSERT the VECTORS into DB\n",
    "def upsertVectors(vectors):\n",
    "    payload = {\n",
    "            \"index_name\": INDEX_NAME,\n",
    "            \"embedded_vectors\": vectors\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{ENDEE_URL}/index/upsert\",\n",
    "            json=payload\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "\n",
    "        print(response.json())\n",
    "        return True\n",
    "\n",
    "    except ConnectionError:\n",
    "        print(\"Backend service is not reachable (is it running?)\")\n",
    "\n",
    "    except Timeout:\n",
    "        print(\"Request timed out\")\n",
    "\n",
    "    except HTTPError as e:\n",
    "        try:\n",
    "            err = e.response.json()\n",
    "            print(\"Error message:\", err.get(\"error\"))\n",
    "        except ValueError:\n",
    "            print(\"Raw error:\", e.response.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30dd914",
   "metadata": {},
   "source": [
    "### Retrieving the Top K most relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d460756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is Manav Khurana\"\n",
    "embedding_for_query = embeddingModel.encode(query).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8057467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"index_name\": INDEX_NAME,\n",
    "    \"vector\": embedding_for_query,\n",
    "    \"top_k\": 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a2d90789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sends a query to the Endee API\n",
    "response = requests.post(\n",
    "    f\"{ENDEE_URL}/index/query\",\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "726dac08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'description': None,\n",
       "  'distance': 0.7324415445327759,\n",
       "  'id': '1780',\n",
       "  'similarity': 0.2675584554672241,\n",
       "  'text': 'GitLab KPIs are duplicates of goals of the reports further down this page.\\nGitLab KPIs are the 10 most important indicators of company performance, and the most important KPI is Net ARR.\\nWe review these at each quarterly meeting of the Board of Directors.\\nThese KPIs are determined by a combination of their stand alone importance to the company and the amount of management focus devoted to improving the metric.',\n",
       "  'title': 'KPIs'},\n",
       " {'description': None,\n",
       "  'distance': 0.7402098476886749,\n",
       "  'id': '3202',\n",
       "  'similarity': 0.2597901523113251,\n",
       "  'text': \"- [Company KPI's](/handbook/company/kpis/)\\n   - [Mitigating Concerns](https://internal.gitlab.com/handbook/leadership/mitigating-concerns/)\",\n",
       "  'title': 'Board of Directors and Corporate Governance'},\n",
       " {'description': 'This page aggregates dashboards, analysis, and insights generated or owned by the Product Data Insights team.',\n",
       "  'distance': 0.7489105761051178,\n",
       "  'id': '3747',\n",
       "  'similarity': 0.2510894238948822,\n",
       "  'text': '### Dashboards\\n\\n[Here is a list of Tableau dashboards](https://10az.online.tableau.com/#/site/gitlab/search/workbooks?search=peterson%20hervas%20raisinghani%20fergen%20petersen%20fisher%20deng%20braza)\\nowned by the Product Data Insights team. Note:',\n",
       "  'title': 'PDI Dashboards, Analysis, & Insights'}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"results\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce9d16f",
   "metadata": {},
   "source": [
    "### Build complete RAG Pipeline:\n",
    "\n",
    "1. Using GROQ Cloud for it's Free APIs and Fast response\n",
    "2. Using Langchain as a warapper\n",
    "\n",
    "### How  does this work:\n",
    "\n",
    "1. `retriever`: returns `List[Document]` for a query (Find relevant information)\n",
    "2. `prompt`: a prompt template with `{context}` and `{input}` (Frame the question for the AI)\n",
    "3. `chatModel`: an LLM runnable (Generate the answer)\n",
    "4. `RunnablePassthrough`: Carry the userâ€™s question forward unchanged\n",
    "5. `StrOutputParser`: Clean the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b93d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the environment variables from the .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "853cf05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "142727ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a professional and a helpful assistant for a Company named GitLab.\n",
    "You answer to all the queries made by the employees of this company to make their work easier.\n",
    "Answer in three to five sentences for general questions.\n",
    "and if the answer needs to be more elaborated, provide more details like step by step instructions only when needed or asked specifically by the user.\n",
    "Use the context given below for your reference and keep the answer concise and to the point.\n",
    "Respond in detail only when the user specifies it.\n",
    "And always perform a Double check before giving the final response to the user.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "If a user ask whether they can upload a Document, respond with \"Yes, you can upload a PDF Document only.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ac4594bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f5daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is wrapper for retrieving the data\n",
    "def endee_retriever(query: str):\n",
    "    embedding_for_query = embeddingModel.encode(query).tolist()\n",
    "\n",
    "    payload = {\n",
    "        \"index_name\": INDEX_NAME,\n",
    "        \"vector\": embedding_for_query,\n",
    "        \"top_k\": 10\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"{ENDEE_URL}/index/query\",\n",
    "        json=payload\n",
    "    )\n",
    "\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    docs = []\n",
    "    for d in data.get(\"matches\", []):\n",
    "        text = d.get(\"text\", \"\")\n",
    "\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"similarity\": d.get(\"similarity\"),\n",
    "                    \"source\": d.get(\"source\"),\n",
    "                    \"title\": d.get(\"title\"),\n",
    "                    \"description\": d.get(\"description\")\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return docs\n",
    "\n",
    "retriever = RunnableLambda(endee_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1e4271d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f93560e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rag pipeline:\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | (lambda docs: \"\\n\\n\".join(d.page_content for d in docs)),\n",
    "        \"input\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c815af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To prepare for an interview, you should learn about the company, the role you're applying for, and the required skills. Review the job description and requirements, and be ready to provide examples of your experience and skills. You should also practice common interview questions and be prepared to ask questions to the interviewer. Additionally, learn about GitLab's products and services, such as Git version control, CI/CD pipelines, and Agile project planning.\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rag_chain.invoke(\"If I want to prepare for an Interview, what all should I need to learn, can you guide me in this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cf8ce6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To prepare for an interview, here's a step-by-step guide:\n",
      "\n",
      "1. **Research the Company**: Learn about GitLab's products, mission, values, and culture to understand our vision and goals.\n",
      "2. **Review the Job Description**: Study the job requirements, responsibilities, and skills needed for the position you're applying for.\n",
      "3. **Update Your Resume and Online Profiles**: Ensure your resume, LinkedIn profile, and other social media platforms are up-to-date and highlight your relevant skills and experiences.\n",
      "4. **Practice Common Interview Questions**: Prepare answers to common interview questions, such as \"Why do you want to work at GitLab?\" or \"What are your strengths and weaknesses?\"\n",
      "5. **Develop Your Technical Skills**: If you're applying for a technical role, review and practice your coding skills, and be prepared to answer technical questions related to your field.\n",
      "6. **Prepare Questions to Ask**: Come up with a list of questions to ask the interviewer, such as \"What are the biggest challenges facing the team right now?\" or \"Can you tell me more about the company culture?\"\n",
      "7. **Mock Interviews**: Practice your interview skills with a friend or family member, or use online resources to simulate a mock interview.\n",
      "8. **Learn About Our Products and Services**: Familiarize yourself with GitLab's products and services, such as GitLab CI/CD, GitLab Runner, and GitLab Pages.\n",
      "9. **Understand Our Values and Culture**: Learn about GitLab's values, such as collaboration, transparency, and results, and be prepared to give examples of how you embody these values.\n",
      "10. **Get Ready Logistically**: Make sure you have a quiet, reliable space for the interview, and test your audio and video equipment beforehand.\n",
      "\n",
      "By following these steps, you'll be well-prepared for your interview and can showcase your skills and experiences to the best of your ability.\n"
     ]
    }
   ],
   "source": [
    "# Sample test:\n",
    "print(rag_chain.invoke(\"If I want to prepare for an Interview, what all should I need to learn, can you guide me in this. Tell me step by step in detail\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e8c206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am a professional and helpful assistant for GitLab, dedicated to providing support and answering queries from employees to make their work easier and more efficient. I am here to assist with any questions or concerns you may have, and I will do my best to provide concise and accurate responses. If you have a specific question or need help with a particular task, feel free to ask and I'll be happy to help. My goal is to provide helpful and timely assistance to ensure a smooth work experience for all GitLab employees.\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Tell me about yourself\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "35c30277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: You can check the company's website for employee referral programs or ask current employees if they can refer you. You can also utilize professional networking platforms like LinkedIn to connect with current or former employees and ask for referrals. Additionally, you can reach out to the company's HR department to inquire about their referral process.\n",
      "Assistant: The company has various job roles, but the specific roles are not mentioned in the given context. If you need more information, please provide more context or clarify which company you are referring to. However, some common job roles in general include administrative, technical, marketing, and sales positions. For more details, please provide additional context.\n",
      "Assistant: You can check the available openings on the official GitLab website, specifically on their careers or jobs page. They list various positions, including engineering, marketing, sales, and more. For the most up-to-date information, I recommend visiting their website directly.\n",
      "Assistant: I am a general assistant, but my specific company affiliation is not provided.\n",
      "Exiting the chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Testing the RAG pipeline with user input in a loop\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting the chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    response = rag_chain.invoke(user_input)\n",
    "    print(f\"Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f99e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
