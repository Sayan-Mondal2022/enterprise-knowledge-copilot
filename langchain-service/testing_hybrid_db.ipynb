{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1298900",
   "metadata": {},
   "source": [
    "## Tesing the Hybrid Index\n",
    "\n",
    "- Using both Sparse Values and Vector Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c47cdacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import (\n",
    "    load_md_with_metadata, \n",
    "    filter_to_minimal_docs,\n",
    "    text_split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e595d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "DATA_FILE_PATH = os.path.join(parent_dir, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89608c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_md_with_metadata(DATA_FILE_PATH)\n",
    "minimal_docs = filter_to_minimal_docs(docs)\n",
    "text_chunks = text_split(minimal_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4006f26",
   "metadata": {},
   "source": [
    "### Sentence-Transformers for VECTOR EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eecf1b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 241.07it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embeddingModel = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95f5e9",
   "metadata": {},
   "source": [
    "### Load a SPLADE model for SPARSE INDICES and VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f764932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sayan\\OneDrive\\Desktop\\internship-project\\enterprise-knowledge-copilot\\langchain-service\\.langchain-venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sayan\\.cache\\huggingface\\hub\\models--naver--splade-cocondenser-ensembledistil. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 204/204 [00:01<00:00, 176.91it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "The tied weights mapping and config for this model specifies to tie bert.embeddings.word_embeddings.weight to cls.predictions.decoder.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie cls.predictions.bias to cls.predictions.decoder.bias, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "BertForMaskedLM LOAD REPORT from: naver/splade-cocondenser-ensembledistil\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"naver/splade-cocondenser-ensembledistil\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"naver/splade-cocondenser-ensembledistil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49cd2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splade_sparse(text, threshold=0.0):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # SPLADE pooling\n",
    "    scores = torch.log1p(torch.relu(logits))\n",
    "    scores = torch.max(scores, dim=1).values.squeeze()\n",
    "\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for idx, score in enumerate(scores):\n",
    "        if score > threshold:\n",
    "            indices.append(idx)\n",
    "            values.append(float(score))\n",
    "\n",
    "    return indices, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7fbe31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero terms: 41\n",
      "Sparse dim: 30522\n"
     ]
    }
   ],
   "source": [
    "indices, values = splade_sparse(\"Machine learning vector database\")\n",
    "\n",
    "print(\"Non-zero terms:\", len(indices))\n",
    "print(\"Sparse dim:\", tokenizer.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f813298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero terms: 53\n",
      "Sparse dim: 30522\n"
     ]
    }
   ],
   "source": [
    "indices, values = splade_sparse(\"Hi I am Sayan and I am trying to build RAG Pipeline\")\n",
    "\n",
    "print(\"Non-zero terms:\", len(indices))\n",
    "print(\"Sparse dim:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1311fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload required to create a HYBRID INDEX\n",
    "\n",
    "INDEX_NAME = \"\"\n",
    "vector_dimensions = embeddingModel.get_sentence_embedding_dimension()\n",
    "sparse_dimensions = tokenizer.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f5e70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
