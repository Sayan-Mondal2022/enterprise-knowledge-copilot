{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1298900",
   "metadata": {},
   "source": [
    "## Tesing the Hybrid Index\n",
    "\n",
    "Using both Sparse Values and Vector Embeddings \n",
    "\n",
    "1. Using SPLADE for computing the Sparse Values and Indices\n",
    "2. Using Sentence-Transformers for computing the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47cdacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sayan\\OneDrive\\Desktop\\internship-project\\enterprise-knowledge-copilot\\langchain-service\\.langchain-venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from doc_preprocessor import (\n",
    "    load_md_with_metadata, \n",
    "    filter_to_minimal_docs,\n",
    "    text_split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e595d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "DATA_FILE_PATH = os.path.join(parent_dir, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89608c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_md_with_metadata(DATA_FILE_PATH)\n",
    "minimal_docs = filter_to_minimal_docs(docs)\n",
    "text_chunks = text_split(minimal_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4006f26",
   "metadata": {},
   "source": [
    "### Sentence-Transformers for VECTOR EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eecf1b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:01<00:00, 92.77it/s, Materializing param=pooler.dense.weight]                              \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embeddingModel = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95f5e9",
   "metadata": {},
   "source": [
    "### Load a SPLADE model for SPARSE INDICES and VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f764932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 204/204 [00:01<00:00, 194.52it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "The tied weights mapping and config for this model specifies to tie bert.embeddings.word_embeddings.weight to cls.predictions.decoder.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie cls.predictions.bias to cls.predictions.decoder.bias, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "BertForMaskedLM LOAD REPORT from: naver/splade-cocondenser-ensembledistil\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"naver/splade-cocondenser-ensembledistil\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"naver/splade-cocondenser-ensembledistil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49cd2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splade_sparse(text, threshold=0.1):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # SPLADE pooling\n",
    "    scores = torch.log1p(torch.relu(logits))\n",
    "    scores = torch.max(scores, dim=1).values.squeeze()\n",
    "\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for idx, score in enumerate(scores):\n",
    "        if score > threshold:\n",
    "            indices.append(idx)\n",
    "            values.append(float(score))\n",
    "\n",
    "    return indices, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09156672",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 2501\n",
    "vectorEmbeddings = []\n",
    "skipped = 0\n",
    "\n",
    "for id, chunk in enumerate(text_chunks[2500:]):\n",
    "    source = chunk.metadata.get('source', \"\")\n",
    "    title = chunk.metadata.get(\"title\", \"\")\n",
    "    description = chunk.metadata.get(\"description\", \"\")\n",
    "    text = chunk.page_content\n",
    "\n",
    "    # Vector Embeddings \n",
    "    embedding = embeddingModel.encode(text).tolist()\n",
    "    if len(embedding) != embeddingModel.get_sentence_embedding_dimension():\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    # Sparse Indices and Values for Hybrid Search\n",
    "    sparse_indices, sparse_values = splade_sparse(text)\n",
    "    if len(sparse_indices) != len(sparse_values):\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    data = {\n",
    "        \"id\": id + start_idx,\n",
    "        \"vector\": embedding,\n",
    "        \"sparse_indices\": sparse_indices,\n",
    "        \"sparse_values\": sparse_values,\n",
    "        \"meta\": {\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"source\": source,\n",
    "            \"text\": text\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(id + 1, \" is been executed\")\n",
    "    \n",
    "    vectorEmbeddings.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6bc83674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 951), (951, 1901), (1901, 2851), (2851, 2992)]\n"
     ]
    }
   ],
   "source": [
    "# Performing list slicing because, insertion limit is 1000 vectors, and I am keeping 950 vectors per upsert\n",
    "slices = []\n",
    "\n",
    "end = len(vectorEmbeddings) // 950\n",
    "prev = 0\n",
    "for i in range(1, end + 1):\n",
    "    slices.append((prev, 950 * i + 1))\n",
    "    prev = 950 * i + 1\n",
    "slices.append((prev, len(vectorEmbeddings)))\n",
    "\n",
    "print(slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e03fc",
   "metadata": {},
   "source": [
    "### Creating the **Hybrid Index** in Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6dbb520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import ConnectionError, Timeout, HTTPError\n",
    "\n",
    "# Index name for Endee Vector Database\n",
    "INDEX_NAME = \"enterprise_knowledge_base2_hybrid\"\n",
    "\n",
    "# URL for Endee API service\n",
    "ENDEE_URL = \"http://127.0.0.1:8000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b4e8123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payload for creating an index in Endee Vector DB\n",
    "payload = {\n",
    "    \"index_name\": INDEX_NAME,\n",
    "    \"dimension\": embeddingModel.get_sentence_embedding_dimension(),\n",
    "    \"sparse_dimension\": tokenizer.vocab_size,\n",
    "    \"precision\": \"INT16D\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c87c737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message for index creation: {'index_name': 'enterprise_knowledge_base2_hybrid', 'status': 'Hybrid index created'}\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    response = requests.post(\n",
    "        f\"{ENDEE_URL}/index/hybrid/create\",\n",
    "        json=payload\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    print(f\"Message for index creation: {response.json()}\")\n",
    "\n",
    "except ConnectionError:\n",
    "    print(\"Backend service is not reachable (is it running?)\")\n",
    "    \n",
    "except Timeout:\n",
    "    print(\"Request timed out\")\n",
    "    \n",
    "except HTTPError as e:\n",
    "    try:\n",
    "        err = e.response.json()\n",
    "        print(\"Error message:\", err.get(\"error\"))\n",
    "    except ValueError:\n",
    "        print(\"Raw error:\", e.response.text)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bbfd6a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index_name': 'enterprise_knowledge_base2_hybrid', 'status': 'index loaded'}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = requests.post(\n",
    "        f\"{ENDEE_URL}/index/get\",\n",
    "        json={\n",
    "            \"index_name\": INDEX_NAME\n",
    "        })\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    print(response.json())\n",
    "    \n",
    "except ConnectionError:\n",
    "    print(\"Backend service is not reachable (is it running?)\")\n",
    "    \n",
    "except Timeout:\n",
    "    print(\"Request timed out\")\n",
    "    \n",
    "except HTTPError as e:\n",
    "    try:\n",
    "        err = e.response.json()\n",
    "        print(\"Error message:\", err.get(\"error\"))\n",
    "    except ValueError:\n",
    "        print(\"Raw error:\", e.response.text)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "948d9396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to UPSERT the VECTORS into DB\n",
    "def upsertVectors(vectors):\n",
    "    payload = {\n",
    "            \"index_name\": INDEX_NAME,\n",
    "            \"embedded_vectors\": vectors\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{ENDEE_URL}/index/hybrid/upsert\",\n",
    "            json=payload\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "\n",
    "        print(response.json())\n",
    "        return True\n",
    "\n",
    "    except ConnectionError:\n",
    "        print(\"Backend service is not reachable (is it running?)\")\n",
    "\n",
    "    except Timeout:\n",
    "        print(\"Request timed out\")\n",
    "\n",
    "    except HTTPError as e:\n",
    "        try:\n",
    "            err = e.response.json()\n",
    "            print(\"Error message:\", err.get(\"error\"))\n",
    "        except ValueError:\n",
    "            print(\"Raw error:\", e.response.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "266a94b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 951, 'status': 'vectors upserted'}\n",
      "{'count': 950, 'status': 'vectors upserted'}\n",
      "{'count': 950, 'status': 'vectors upserted'}\n",
      "{'count': 141, 'status': 'vectors upserted'}\n"
     ]
    }
   ],
   "source": [
    "for start, end in slices:\n",
    "    vectors = vectorEmbeddings[start:end]\n",
    "    upsertVectors(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca8e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'description': 'Deep dive into Customer Experience (CX)', 'distance': 0.9674775265157223, 'id': '1', 'similarity': 0.032522473484277725, 'text': 'Deep dive into Customer Experience (CX) \\n Understanding how this critical practice drives customer-centricity in GitLab \\n What is Customer Experience (CX)?', 'title': 'About Customer Experience (CX)'}, {'description': 'Understanding the current customer experience, identifying opportunities where we can improve, and leveraging metrics to advocate key experience improvement initiatives.', 'distance': 0.9679815582931042, 'id': '942', 'similarity': 0.03201844170689583, 'text': 'Customer Experience (CX) research takes three perspectives into account: \\n \\n Customer Lens: Understanding the customer’s key activities along our core customer journey, their needs, who on their side is engaged in purchase decisions, through to onboarding and adoption, and the needs and expectations of GitLab for success.', 'title': 'Customer Experience Journey Research'}]\n"
     ]
    }
   ],
   "source": [
    "# Sample RETRIEVAL of data relevant to queries.\n",
    "query = \"Deep dive into Customer Experience\"\n",
    "embedding_for_query = embeddingModel.encode(query).tolist()\n",
    "sparse_indices, sparse_values = splade_sparse(query)\n",
    "\n",
    "payload = {\n",
    "    \"index_name\": INDEX_NAME,\n",
    "    \"vector\": embedding_for_query,\n",
    "    \"sparse_indices\": sparse_indices,\n",
    "    \"sparse_values\": sparse_values,\n",
    "    \"top_k\": 20\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Sends a query to the Endee API\n",
    "    response = requests.post(\n",
    "        f\"{ENDEE_URL}/index/hybrid/query\",\n",
    "        json=payload\n",
    "    )\n",
    "\n",
    "    response.raise_for_status()\n",
    "    print(response.json())\n",
    "    \n",
    "except ConnectionError:\n",
    "    print(\"Backend service is not reachable (is it running?)\")\n",
    "    \n",
    "except Timeout:\n",
    "    print(\"Request timed out\")\n",
    "    \n",
    "except HTTPError as e:\n",
    "    try:\n",
    "        err = e.response.json()\n",
    "        print(\"Error message:\", err.get(\"error\"))\n",
    "    except ValueError:\n",
    "        print(\"Raw error:\", e.response.text)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "\n",
    "data = response.json()\n",
    "print(data[\"results\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c601ea8e",
   "metadata": {},
   "source": [
    "### Building the Complete RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a37ff435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the environment variables from the .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3b047a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00158ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5280d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is wrapper for retrieving the data\n",
    "def endee_retriever(query: str):\n",
    "    embedding_for_query = embeddingModel.encode(query).tolist()\n",
    "\n",
    "    payload = {\n",
    "        \"index_name\": INDEX_NAME,\n",
    "        \"vector\": embedding_for_query,\n",
    "        \"top_k\": 20\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"{ENDEE_URL}/index/hybrid/query\",\n",
    "        json=payload\n",
    "    )\n",
    "\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    docs = []\n",
    "    for d in data.get(\"results\", []):\n",
    "        text = d.get(\"text\", \"\")\n",
    "\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"similarity\": d.get(\"similarity\"),\n",
    "                    \"source\": d.get(\"source\"),\n",
    "                    \"title\": d.get(\"title\"),\n",
    "                    \"description\": d.get(\"description\")\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return docs\n",
    "\n",
    "retriever = RunnableLambda(endee_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3888dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a professional and a helpful virtual assistant for a Company named GitLab.\n",
    "Your name is 'GitLab Copilot' and your job is to answer to all the queries made by the employees of this company to make their work easier.\n",
    "Answer in three to five sentences for general questions.\n",
    "and if the answer needs to be more elaborated, provide more details like step by step instructions only when needed or asked specifically by the user.\n",
    "Use the context given below for your reference and keep the answer concise and to the point.\n",
    "Respond in detail only when the user specifies it.\n",
    "And always perform a Double check before giving the final response to the user.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "If a user ask whether they can upload a Document, respond with \"Yes, you can upload a PDF Document only.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9710ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Rag pipeline:\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | (lambda docs: \"\\n\\n\".join(d.page_content for d in docs)),\n",
    "        \"input\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "32b1f2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To prepare for the interview, it's essential to familiarize yourself with the job description and requirements. Review the job family page and reach out to the Hiring Manager if you have any questions. Align with the Hiring Manager on competencies and prepare actionable steps to work on those areas. You can also review the interview kit in Greenhouse, which includes the candidate's resume, interview description, scorecard, and suggested questions. The hiring process typically involves several stages, including Application, Technical Interview, and Hiring Manager Interview, followed by a Senior Manager/Director/VP Interview for successful candidates.\n"
     ]
    }
   ],
   "source": [
    "# Sample test:\n",
    "print(rag_chain.invoke(\"How can I prepare for the interview, can you also tell the stages involved in Hiring process\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
